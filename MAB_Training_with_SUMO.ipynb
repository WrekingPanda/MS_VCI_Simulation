{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa52ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import traci\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import sumolib\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define your local SUMO bin path (Check if this matches your actual installation path)\n",
    "sumo_home = \"C:/Program Files (x86)/Eclipse/Sumo\"\n",
    "\n",
    "tools_path = os.path.join(sumo_home, 'tools')\n",
    "\n",
    "if tools_path not in sys.path:\n",
    "    sys.path.append(tools_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb753e0d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your sumo.exe path, for faster training without the gui\n",
    "SUMO_BINARY = \".../sumo.exe\"  # or \"sumo-gui\"\n",
    "SUMO_CFG = \"vci.sumocfg\"\n",
    "\n",
    "# Possible origins\n",
    "START_EDGES_C = [\"1019723\",\"1019718\",\"1063513\",\"1062943\",\"1020382\",\"2842002\",\"1020167\",\"1020160\",\"2842000\",\"1516937\",\"1016643\",\"1020050\",\"1020045\",\"1019568\",\"1210462\",\"1582715\",\"1405109\",\"1302643\",\"1038024\",\"1215193\",\"2016241\",\"1810069\",\"1204005\",\"1401473\",\"1204022\",\"1189928\"]\n",
    "START_EDGES_D = [\"1189910\",\"1401479\",\"1175990\",\"1111269.111\",\"1111267\",\"1215241\",\"1181539\",\"1111242\",\"1216119\",\"1122615\",\"1016648\",\"1188031\",\"1255432\",\"1020165\",\"1768757\",\"1949246\",\"1306152\",\"1062246\",\"1122691\",\"1019722\",\"1019716\",\"1401214\"]\n",
    "# Possible destinations\n",
    "END_EDGES_C = [\"1019719\",\"1122692\",\"1063262\",\"1047385\",\"1076349\",\"1016658\",\"1020171\",\"1020157\",\"1888721\",\"1888720\",\"2020471\",\"1016644\",\"1020046\",\"1020029\",\"1019569\",\"1210226\",\"1111488\",\"1036157\",\"1016681\",\"1215220\",\"1935065\",\"1175980\",\"1401472\",\"1189936\",\"1210069\"]\n",
    "END_EDGES_D = [\"1401537\",\"1189937\",\"1203952\",\"1810067\",\"1111269\",\"1188014\",\"1215352\",\"1181481\",\"1111240\",\"1214240\",\"1019567\",\"1020004\",\"2006681\",\"1019667\",\"1020161\",\"1020174\",\"1016657\",\"1054906\",\"1047469\",\"1062317\",\"1063338\",\"1019717\",\"1051044\",\"1972190\"]\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(\"./Dataset/simple_train.csv\", parse_dates=[\"AGG_PERIOD_START\"])\n",
    "test_df = pd.read_csv(\"./Dataset/simple_test.csv\", parse_dates=[\"AGG_PERIOD_START\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c791c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_half_arm(df):\n",
    "    if isinstance(df[\"half_arm\"].iloc[0], tuple):\n",
    "        return df\n",
    "\n",
    "    def parse_arm(s):\n",
    "        return eval(s)\n",
    "    df = df.copy()\n",
    "    df[\"half_arm\"] = df[\"half_arm\"].apply(parse_arm)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d4e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the arm keys are tuples\n",
    "train_df = ensure_half_arm(train_df)\n",
    "test_df = ensure_half_arm(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa2d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df[\"AGG_PERIOD_START\"].dt.date\n",
    "    return df\n",
    "\n",
    "def get_real_counts_for_day(df_day):\n",
    "    g = df_day.groupby(\"half_arm\")[\"TOTAL_VOLUME\"].sum()\n",
    "    return g.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61119e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the real sensors to the SUMO induction loop detectors introduced by us\n",
    "sensor_id_to_detectors = {121726: [\"121726_0\", \"121726_1\", \"121726_2\"],\n",
    "                          121727: [\"121727_0\", \"121727_1\", \"121727_2\"],\n",
    "                          121731: [\"121731_0\", \"121731_1\", \"121731_2\", \"121731_3\"],\n",
    "                          121732: [\"121732_0\", \"121732_1\", \"121732_2\", \"121732_3\"],\n",
    "                          121733: [\"121733_0\", \"121733_1\", \"121733_2\"],\n",
    "                          121734: [\"121734_0\", \"121734_1\", \"121734_2\", \"121734_3\"],\n",
    "                          121735: [\"121735_0\", \"121735_1\", \"121735_2\"],\n",
    "                          121736: [\"121736_0\", \"121736_1\", \"121736_2\"],\n",
    "                          121741: [\"121741_0\", \"121741_1\", \"121741_2\"],\n",
    "                          121742: [\"121742_0\", \"121742_1\", \"121742_2\"],\n",
    "                          121754: [\"121754_0\", \"121754_1\"],\n",
    "                          121755: [\"121755_0\", \"121755_1\"],\n",
    "                          121756: [\"121756_0\", \"121756_1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80fdc6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the real sensors to the direction of the road they monitor ('C' or 'D')\n",
    "sensor_direction = {121726: 'D',\n",
    "                    121727: 'D',\n",
    "                    121731: 'C',\n",
    "                    121732: 'C',\n",
    "                    121733: 'C',\n",
    "                    121734: 'C',\n",
    "                    121735: 'C',\n",
    "                    121736: 'C',\n",
    "                    121741: 'C',\n",
    "                    121742: 'C',\n",
    "                    121754: 'D',\n",
    "                    121755: 'C',\n",
    "                    121756: 'C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373b9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the sensors to the road segment they're on\n",
    "sensor_to_edge = {\"121726\": \"1254022\",\n",
    "                  \"121727\": \"1214665\",\n",
    "                  \"121731\": \"1063682\",\n",
    "                  \"121732\": \"1062244\",\n",
    "                  \"121733\": \"1062753\",\n",
    "                  \"121734\": \"1051025\",\n",
    "                  \"121735\": \"1051026\",\n",
    "                  \"121736\": \"1062939\",\n",
    "                  \"121741\": \"1254019\",\n",
    "                  \"121742\": \"1254017\",\n",
    "                  \"121754\": \"1175979\",\n",
    "                  \"121755\": \"1176005\",\n",
    "                  \"121756\": \"1181559\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4098e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sumo_detector_output(xml_file, det_lookup):\n",
    "    \"\"\"\n",
    "    Reads the SUMO output file and aggregates counts.\n",
    "    \n",
    "    Args:\n",
    "        xml_file: Path to the detector output (e.g., \"detectors.out.xml\")\n",
    "        det_lookup: Dict mapping { \"sumo_det_id\": (sensor_id, direction) }\n",
    "        \n",
    "    Returns:\n",
    "        sim_counts: { (half_bin, sensor_id, direction): count }\n",
    "    \"\"\"\n",
    "    sim_counts = defaultdict(int)\n",
    "    \n",
    "    # Parse the XML file\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for interval in root.findall('interval'):\n",
    "            det_id = interval.get('id')\n",
    "            \n",
    "            # Only process if this is one of our known sensors\n",
    "            if det_id in det_lookup:\n",
    "                # Get the count for this interval\n",
    "                count = int(interval.get('nVehContrib', 0))\n",
    "                \n",
    "                if count > 0:\n",
    "                    sensor_id, direction = det_lookup[det_id]\n",
    "                    \n",
    "                    # Calculate the time bin\n",
    "                    # 'begin' is in seconds (e.g., 0.00, 1800.00)\n",
    "                    begin_time = float(interval.get('begin'))\n",
    "                    half_bin = int(begin_time // 1800)\n",
    "                    \n",
    "                    # Add to our aggregate\n",
    "                    arm_key = (half_bin, sensor_id, direction)\n",
    "                    sim_counts[arm_key] += count\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Could not find {xml_file}. Did SUMO crash?\")\n",
    "        \n",
    "    return dict(sim_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3a5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the map network\n",
    "net = sumolib.net.readNet(\"net.net.xml\")\n",
    "\n",
    "def get_connected_edges(sensor_id, depth=3, direction=\"upstream\"):\n",
    "    \"\"\"\n",
    "    Finds N edges either 'behind' (upstream) or 'after' (downstream) the sensor.\n",
    "    \"\"\"\n",
    "    sensor_edge_id = sensor_to_edge[sensor_id]\n",
    "    sensor_edge = net.getEdge(sensor_edge_id)\n",
    "    found_edges = []\n",
    "    queue = [sensor_edge]\n",
    "    visited = {sensor_edge_id}\n",
    "    \n",
    "    while queue and len(found_edges) < depth:\n",
    "        current_edge = queue.pop(0)\n",
    "        \n",
    "        # Determine which neighbors to look at\n",
    "        if direction == \"upstream\":\n",
    "            neighbors = current_edge.getFromNode().getIncoming()\n",
    "        else: # downstream\n",
    "            neighbors = current_edge.getToNode().getOutgoing()\n",
    "            \n",
    "        for n_edge in neighbors:\n",
    "            n_id = n_edge.getID()\n",
    "            if n_id not in visited:\n",
    "                found_edges.append(n_id)\n",
    "                visited.add(n_id)\n",
    "                queue.append(n_edge)\n",
    "    return found_edges if found_edges else [sensor_edge_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4766c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sensor_edge_mapping():\n",
    "    mapping = {}\n",
    "    sensor_direction = {121726: 'D', 121727: 'D', 121731: 'C', 121732: 'C', 121733: 'C', 121734: 'C', 121735: 'C', 121736: 'C', 121741: 'C', 121742: 'C', 121754: 'D', 121755: 'C', 121756: 'C'}\n",
    "\n",
    "    for s_id, direction in sensor_direction.items():\n",
    "        sensor_id = str(s_id)\n",
    "        edge_id = sensor_to_edge[sensor_id]\n",
    "        \n",
    "        # Get 3 edges before the sensor\n",
    "        possible_starts = get_connected_edges(sensor_id, depth=3, direction=\"upstream\")\n",
    "        \n",
    "        # Get 7 edges after the sensor\n",
    "        possible_ends = get_connected_edges(sensor_id, depth=7, direction=\"downstream\")\n",
    "        \n",
    "        mapping[(s_id, direction)] = (possible_starts, edge_id, possible_ends)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple independent Gaussian Thompson Sampling per arm.\n",
    "# Reward = -loss; here we will approximate per-arm reward using negative squared error between sim and real volume.\n",
    "class GaussianThompsonBandit:\n",
    "    def __init__(self, arms, init_mean=20.0, init_var=100.0, obs_noise_var=100.0):\n",
    "        self.arms = list(arms)\n",
    "        self.obs_noise_var = obs_noise_var\n",
    "        self.means = {a: init_mean for a in self.arms}\n",
    "        self.vars  = {a: init_var  for a in self.arms}\n",
    "\n",
    "    def sample_flows(self):\n",
    "        flows = {}\n",
    "        MAX_FLOW = 7000.0\n",
    "        for a in self.arms:\n",
    "            mu = self.means[a]\n",
    "            var = self.vars[a]\n",
    "\n",
    "            # Sample from the normal distribution\n",
    "            sample = np.random.normal(mu, np.sqrt(var))\n",
    "\n",
    "            # Clip the value to ensure it stays within physically possible limits\n",
    "            flows[a] = np.clip(sample, 0.0, MAX_FLOW)\n",
    "        return flows\n",
    "\n",
    "    def update(self, real_counts, sim_counts, reward=None):\n",
    "        # Pick one arm to monitor (Optional)\n",
    "        debug_arm = next(iter(self.arms)) if self.arms else None\n",
    "        before = (self.means.get(debug_arm), self.vars.get(debug_arm)) if debug_arm else None\n",
    "\n",
    "        updated_count = 0\n",
    "        for a, y in real_counts.items():\n",
    "            if a in self.means:\n",
    "                mu_prior = self.means[a]\n",
    "                var_prior = self.vars[a]\n",
    "                var_noise = self.obs_noise_var\n",
    "\n",
    "                # Bayesian update for normal-normal model\n",
    "                var_post = 1.0 / (1.0 / var_prior + 1.0 / var_noise)\n",
    "                mu_post = var_post * (mu_prior / var_prior + y / var_noise)\n",
    "\n",
    "                self.means[a] = mu_post\n",
    "                self.vars[a]  = var_post\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                # This will trigger if the arm keys don't match\n",
    "                print(f\"[DEBUG] Key mismatch: {a} not found in bandit arms!\")\n",
    "        print(f\"[BANDIT] Successfully updated {updated_count} out of {len(real_counts)} real counts.\")\n",
    "        \n",
    "    def sample_flows_subset(self, arms_subset):\n",
    "        MAX_FLOW = 7000.0\n",
    "        flows = {}\n",
    "        for a in arms_subset:\n",
    "            mu = self.means[a]\n",
    "            var = self.vars[a]\n",
    "            flows[a] = np.clip(\n",
    "                np.random.normal(mu, np.sqrt(var)), 0.0, MAX_FLOW\n",
    "            )\n",
    "        return flows\n",
    "\n",
    "    def save(self, filepath):\n",
    "        data = {\n",
    "            'means': self.means,\n",
    "            'vars': self.vars,\n",
    "            'arms': self.arms,\n",
    "            'obs_noise_var': self.obs_noise_var\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"[INFO] Model saved to {filepath}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Reconstruct the bandit with saved state\n",
    "        bandit = GaussianThompsonBandit(data['arms'], obs_noise_var=data['obs_noise_var'])\n",
    "        bandit.means = data['means']\n",
    "        bandit.vars = data['vars']\n",
    "        print(f\"[INFO] Model loaded from {filepath}\")\n",
    "        return bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c297ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trips_for_day(flow_estimates, sensor_edge_map, time_bins_per_arm):\n",
    "    trips = []\n",
    "    v_idx = 0\n",
    "    for arm, flow_val in flow_estimates.items():\n",
    "        half_bin, sensor_id, direction = arm\n",
    "        FLOW_SCALE = 0.1  # simulate 10%\n",
    "\n",
    "        n = int(round(flow_val * FLOW_SCALE))\n",
    "        \n",
    "        if n == 0 or (sensor_id, direction) not in sensor_edge_map:\n",
    "            continue\n",
    "\n",
    "        possible_starts, sensor_edge, possible_ends = sensor_edge_map[(sensor_id, direction)]\n",
    "        t0, t1 = time_bins_per_arm[arm]\n",
    "\n",
    "        for _ in range(n):\n",
    "            # Randomly pick from the verified local edges\n",
    "            start_node = random.choice(possible_starts)\n",
    "            end_node = random.choice(possible_ends)\n",
    "            \n",
    "            depart = random.uniform(t0, t1)\n",
    "            veh_id = f\"veh_{v_idx}\"\n",
    "            \n",
    "            # (veh_id, depart, from, via, to)\n",
    "            trips.append((veh_id, depart, start_node, sensor_edge, end_node))\n",
    "            v_idx += 1\n",
    "    return trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_bins_for_half_hour():\n",
    "    mapping = {}\n",
    "    for hb in range(48):\n",
    "        start_min = hb * 30\n",
    "        end_min   = start_min + 30\n",
    "        mapping[hb] = (start_min * 60, end_min * 60)\n",
    "    return mapping\n",
    "\n",
    "def build_time_bins_per_arm(arms):\n",
    "    half_bin_map = build_time_bins_for_half_hour()\n",
    "    return {a: half_bin_map[a[0]] for a in arms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trips_xml(trips, filename=\"daily_trips.rou.xml\"):\n",
    "    # Trips need to be ordered by departure time\n",
    "    trips.sort(key=lambda x: x[1])\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write('<routes>\\n')\n",
    "        # Define a standard vehicle type\n",
    "        f.write('    <vType id=\"car\" accel=\"2.6\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"70\"/>\\n')\n",
    "        \n",
    "        for veh_id, depart, start_edge, end_edge, arm in trips:\n",
    "            # Write trip directly to XML\n",
    "            f.write(f'    <vehicle id=\"{veh_id}\" type=\"car\" depart=\"{depart}\" departLane=\"best\" departSpeed=\"max\">\\n')\n",
    "            f.write(f'        <route edges=\"{start_edge} {end_edge}\"/>\\n')\n",
    "            f.write('    </vehicle>\\n')\n",
    "            \n",
    "        f.write('</routes>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9cb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sumo_episode(trips, sensor_edge_map, sensor_id_to_detectors):\n",
    "    print(f\"[SUMO] Starting episode with {len(trips)} trips.\")\n",
    "    \n",
    "    # Generate Route File\n",
    "    route_filename = \"daily_trips.rou.xml\"\n",
    "    write_trips_xml(trips, route_filename)\n",
    "\n",
    "    # Configure SUMO\n",
    "    sumo_cmd = [\n",
    "        SUMO_BINARY, \n",
    "        \"-c\", SUMO_CFG,\n",
    "        \"-r\", route_filename,\n",
    "        \"--mesosim\", \"true\",\n",
    "        \"--meso-overtaking\", \"false\",\n",
    "        \"--step-length\", \"2\",\n",
    "        \"--no-step-log\", \"true\",\n",
    "        \"--log\", \"sumo_error.log\",        # Check this file if it crashes!\n",
    "        \"--ignore-route-errors\", \"true\",\n",
    "        \"--quit-on-end\", \"true\"\n",
    "    ]\n",
    "\n",
    "    # Start TraCI (Automatic Port Selection)\n",
    "    traci.start(sumo_cmd)\n",
    "    # Wait briefly for connection to stabilize\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    sim_counts = defaultdict(int)\n",
    "\n",
    "    try:\n",
    "        # Subscribe to Detectors\n",
    "        unique_sensors = {arm[1] for _, _, _, _, arm in trips}\n",
    "        for sensor_id in unique_sensors:\n",
    "            det_ids = sensor_id_to_detectors.get(sensor_id, [])\n",
    "            for det_id in det_ids:\n",
    "                # 0x10 is the constant for LAST_STEP_VEHICLE_NUMBER\n",
    "                traci.inductionloop.subscribe(det_id, [0x10])\n",
    "        \n",
    "        # Simulation Loop\n",
    "        while traci.simulation.getMinExpectedNumber() > 0:\n",
    "            traci.simulationStep()\n",
    "            \n",
    "            # Retrieve subscription results efficiently\n",
    "            for sensor_id in unique_sensors:\n",
    "                det_ids = sensor_id_to_detectors.get(sensor_id, [])\n",
    "                for det_id in det_ids:\n",
    "                    res = traci.inductionloop.getSubscriptionResults(det_id)\n",
    "                    if res and (0x10 in res):\n",
    "                        count = res[0x10]\n",
    "                        if count > 0:\n",
    "                            pass \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[SUMO ERROR] {e}\")\n",
    "        # If it crashes, print the log file content\n",
    "        if os.path.exists(\"sumo_error.log\"):\n",
    "            with open(\"sumo_error.log\", \"r\") as f:\n",
    "                print(\"--- SUMO LOG START ---\")\n",
    "                print(f.read())\n",
    "                print(\"--- SUMO LOG END ---\")\n",
    "        raise e\n",
    "\n",
    "    finally:\n",
    "        traci.close()\n",
    "        time.sleep(1.0) # Give OS time to release the port\n",
    "        print(\"[SUMO] Connection closed.\")\n",
    "\n",
    "    # Parse Results from File\n",
    "    print(\"[SUMO] Reading results from XML...\")\n",
    "    det_lookup = {}\n",
    "    for s_id, det_list in sensor_id_to_detectors.items():\n",
    "        d = sensor_direction.get(s_id)\n",
    "        for det_id in det_list:\n",
    "            det_lookup[det_id] = (s_id, d)\n",
    "            \n",
    "    sim_counts = parse_sumo_detector_output(\"detectors.out.xml\", det_lookup)\n",
    "    \n",
    "    return sim_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc25e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(sim_counts, real_counts, loss_type=\"l1\"):\n",
    "    loss = 0.0\n",
    "\n",
    "    # Union of arms present in either dict\n",
    "    all_arms = set(real_counts.keys()) | set(sim_counts.keys())\n",
    "\n",
    "    for arm in all_arms:\n",
    "        y_real = real_counts.get(arm, 0.0)\n",
    "        y_sim  = sim_counts.get(arm, 0.0)\n",
    "        diff = y_sim - y_real\n",
    "\n",
    "        if loss_type == \"l2\":\n",
    "            loss += diff ** 2\n",
    "        else:  # \"l1\"\n",
    "            loss += abs(diff)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75704769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bandit_with_sumo(train_df, num_episodes=None):\n",
    "    # Ensure half_arm is a tuple\n",
    "    train_df = ensure_half_arm(train_df)\n",
    "    train_df = add_date(train_df)\n",
    "\n",
    "    # Arms from training data\n",
    "    arms = sorted(train_df[\"half_arm\"].unique())\n",
    "    bandit = GaussianThompsonBandit(arms)\n",
    "\n",
    "    # Mapping from (sensor, dir) to (start_edge, end_edge)\n",
    "    sensor_edge_map = build_sensor_edge_mapping()\n",
    "    \n",
    "    # # Maps the real sensors to the SUMO induction loop detectors introduced by us\n",
    "    # sensor_id_to_detectors = {121726: [\"121726_0\", \"121726_1\", \"121726_2\"],\n",
    "    #                         121727: [\"121727_0\", \"121727_1\", \"121727_2\"],\n",
    "    #                         121731: [\"121731_0\", \"121731_1\", \"121731_2\", \"121731_3\"],\n",
    "    #                         121732: [\"121732_0\", \"121732_1\", \"121732_2\", \"121732_3\"],\n",
    "    #                         121733: [\"121733_0\", \"121733_1\", \"121733_2\"],\n",
    "    #                         121734: [\"121734_0\", \"121734_1\", \"121734_2\", \"121734_3\"],\n",
    "    #                         121735: [\"121735_0\", \"121735_1\", \"121735_2\"],\n",
    "    #                         121736: [\"121736_0\", \"121736_1\", \"121736_2\"],\n",
    "    #                         121741: [\"121741_0\", \"121741_1\", \"121741_2\"],\n",
    "    #                         121742: [\"121742_0\", \"121742_1\", \"121742_2\"],\n",
    "    #                         121754: [\"121754_0\", \"121754_1\"],\n",
    "    #                         121755: [\"121755_0\", \"121755_1\"],\n",
    "    #                         121756: [\"121756_0\", \"121756_1\"]}\n",
    "\n",
    "    # Time-bin mapping\n",
    "    time_bins_per_arm = build_time_bins_per_arm(arms)\n",
    "\n",
    "    # Group by day\n",
    "    grouped = train_df.groupby(\"date\")\n",
    "    days = list(grouped.groups.keys())\n",
    "    if num_episodes is not None:\n",
    "        days = days[:num_episodes]\n",
    "\n",
    "    print(f\"[INFO] Training on {len(days)} days, {len(arms)} arms.\")\n",
    "\n",
    "    for day_idx, day in enumerate(days, start=1):\n",
    "        df_day = grouped.get_group(day)\n",
    "        print(f\"\\n[DAY {day_idx}/{len(days)}] {day}\")\n",
    "\n",
    "        # Real counts per arm for this day\n",
    "        real_counts = get_real_counts_for_day(df_day)\n",
    "        print(f\"[DAY {day}] unique arms today: {len(real_counts)}\")\n",
    "\n",
    "        # Bandit samples flow estimates per arm\n",
    "        active_arms = real_counts.keys()\n",
    "        flow_estimates = bandit.sample_flows_subset(active_arms)\n",
    "        print(f\"[DAY {day}] sampled flows (first 5): {list(flow_estimates.items())[:5]}\")\n",
    "\n",
    "        # Build trips for this day from flow estimates\n",
    "        trips = build_trips_for_day(flow_estimates, sensor_edge_map, time_bins_per_arm)\n",
    "        print(f\"[DAY {day}] generated {len(trips)} trips.\")\n",
    "\n",
    "        # Run SUMO and get simulated counts per arm\n",
    "        sim_counts = run_sumo_episode(trips, sensor_edge_map, sensor_id_to_detectors)\n",
    "        \n",
    "        # Compute scaled loss for logging and reward\n",
    "        # Instead of a raw sum, use the average error per arm (MAE)\n",
    "        raw_loss = compute_loss(sim_counts, real_counts, loss_type=\"l1\")\n",
    "\n",
    "        # Scaling Factor: Normalize by the number of arms tracked today\n",
    "        num_arms_today = len(real_counts)\n",
    "        if num_arms_today > 0:\n",
    "            mae = raw_loss / num_arms_today\n",
    "        else:\n",
    "            mae = 0\n",
    "\n",
    "        # Reward Transformation (Using a scaling factor keeps the reward in a smaller range, which prevents the Bandit's variance from collapsing or exploding)\n",
    "        reward = -(mae * 0.1)\n",
    "        print(f\"[DAY {day}] Total Loss: {raw_loss:.2f} | MAE: {mae:.2f} | Scaled Reward: {reward:.2f}\")\n",
    "\n",
    "        # Update bandit with real and simulated counts\n",
    "        bandit.update(real_counts, sim_counts, reward)\n",
    "        print(f\"[DAY {day}] bandit updated.\")\n",
    "\n",
    "        # Temporary save each 10 days\n",
    "        if day_idx%10 == 0:\n",
    "            bandit.save(\"vci_bandit_model.pkl\")\n",
    "\n",
    "    return bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if a running traci session is already detected\n",
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"Start Training\")\n",
    "trained_bandit = train_bandit_with_sumo(train_df, 730)\n",
    "\n",
    "# Save the final model\n",
    "trained_bandit.save(\"vci_bandit_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
